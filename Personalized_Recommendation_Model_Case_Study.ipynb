{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed367a-5b7a-43ce-b135-bd76bf93d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Case Study: Building a Personalized Recommendation Model\n",
    "\n",
    "This notebook presents the process of developing a personalized recommendation model based on user behaviour data from a shopping website.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d8fc1-43f5-40e0-b3a5-e6720c1e0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Processing Module\n",
    "\n",
    "#Imports\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "#Loading Data\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Loads data from a Parquet file.\n",
    "\n",
    "    Args:\n",
    "    filepath (str): Path to the Parquet file.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Loaded DataFrame.\n",
    "    \"\"\"\n",
    "    return pd.read_parquet(filepath)\n",
    "    \n",
    "\n",
    "#Session-based aggregation\n",
    "def aggregate_session_data(data):\n",
    "    \"\"\"\n",
    "    Aggregates session-based data.\n",
    "\n",
    "    Args:\n",
    "    data (pd.DataFrame): Raw data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Aggregated session data with additional features.\n",
    "    \"\"\"\n",
    "    # Convert timestamp to datetime\n",
    "    data['timestamp'] = pd.to_datetime(data['date'])\n",
    "\n",
    "    # Group by session ID and aggregate data\n",
    "    session_data = data.groupby('sessionId').agg({\n",
    "        'userId': 'first',\n",
    "        'timestamp': ['min', 'max'],\n",
    "        'pageType': lambda x: list(x),\n",
    "        'itemId': lambda x: list(x),\n",
    "        'productPrice': lambda x: list(x),\n",
    "        'oldProductPrice': lambda x: list(x),\n",
    "        'category': lambda x: list(x)\n",
    "    }).reset_index()\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    session_data.columns = ['sessionId', 'userId', 'session_start', 'session_end', 'pageTypes', 'itemIds',\n",
    "                            'productPrices', 'oldProductPrices', 'categories']\n",
    "\n",
    "    # Calculate session duration in seconds\n",
    "    session_data['session_duration'] = (session_data['session_end'] - session_data['session_start']).dt.total_seconds()\n",
    "\n",
    "    # Calculate number of events per session\n",
    "    session_data['num_events'] = session_data['pageTypes'].apply(len)\n",
    "\n",
    "    # Additional labels for recommendation:\n",
    "    # 1. Interaction label based on 'success' in pageTypes\n",
    "    session_data['interaction'] = session_data['pageTypes'].apply(lambda x: 1 if 'success' in x else 0)\n",
    "\n",
    "    # 2. Average product price per session\n",
    "    session_data['avg_product_price'] = session_data['productPrices'].apply(lambda x: sum(x) / len(x) if len(x) > 0 else 0)\n",
    "\n",
    "    return session_data\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATA_FILE_PATH = \"path/to/data/file.parquet\"\n",
    "\n",
    "# Load data\n",
    "data = load_data(DATA_FILE_PATH)\n",
    "\n",
    "# Aggregate session data\n",
    "session_data = aggregate_session_data(data)\n",
    "\n",
    "# Display the first few rows of aggregated session data\n",
    "session_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2bea18-f3ac-460a-8ca0-eee8286e02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Feature Engineering\n",
    "#feature_engineering module is focused on creating mappings for user and item IDs, \n",
    "#and generating features for model training based on these mappings. \n",
    "\n",
    "#creating user and item mappings\n",
    "def create_user_item_mappings(data):\n",
    "    \"\"\"\n",
    "    Create mappings for user and item IDs to integer indices.\n",
    "\n",
    "    Args:\n",
    "    data (pd.DataFrame): Session-based aggregated data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (user_mapping, item_mapping)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    user_ids = data['userId'].unique()\n",
    "    item_ids = data['itemId'].explode().unique()\n",
    "    \n",
    "\n",
    "    #creating userId - userId_encoded mapping\n",
    "    user_mapping = pd.DataFrame({\n",
    "        'userId': user_ids,\n",
    "        'userId_encoded': np.arange(len(user_ids))\n",
    "    })\n",
    "    \n",
    "    #creating itemId - itemId_encoded mapping\n",
    "    item_mapping = pd.DataFrame({\n",
    "        'itemId': item_ids,\n",
    "        'itemId_encoded': np.arange(len(item_ids))\n",
    "    })\n",
    "\n",
    "    return user_mapping, item_mapping\n",
    "\n",
    "#generating features\n",
    "def generate_features(data, user_mapping, item_mapping):\n",
    "    \"\"\"\n",
    "    Generate features for model training.\n",
    "\n",
    "    Args:\n",
    "    data (pd.DataFrame): Session-based aggregated data.\n",
    "    user_mapping (pd.DataFrame): Mapping of user IDs to integer indices.\n",
    "    item_mapping (pd.DataFrame): Mapping of item IDs to integer indices.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (X, y)\n",
    "    \"\"\"\n",
    "    # Merge user mapping\n",
    "    data = data.merge(user_mapping, left_on='userId', right_on='userId', how='left')\n",
    "    \n",
    "    # Explode itemId column to ensure each item is on a separate row\n",
    "    data = data.explode('itemId')\n",
    "\n",
    "    # Merge item mapping\n",
    "    data = data.merge(item_mapping, left_on='itemId', right_on='itemId', how='left')\n",
    "\n",
    "    # Select features and target\n",
    "    X = data[['userId_encoded', 'itemId_encoded']].values\n",
    "    y = data['interaction'].values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "    #Create mappings\n",
    "    user_mapping, item_mapping = create_user_item_mappings(session_data)\n",
    "\n",
    "    #Genereate features\n",
    "    X, y = generate_features(session_data, user_mapping, item_mapping)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e5033-be83-4582-91b0-b2cdb57ac7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Build and Train Model Module\n",
    "\n",
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#Define the build_model function to construct a recommendation model using TensorFlow. \n",
    "#This function sets up embedding layers for users and items, concatenates them, and adds dense layers for learning interactions.\n",
    "def build_model(num_users, num_items, embedding_dim=50):\n",
    "    \"\"\"\n",
    "    Build the recommendation model using TensorFlow.\n",
    "\n",
    "    Args:\n",
    "    num_users (int): Number of unique users.\n",
    "    num_items (int): Number of unique items.\n",
    "    embedding_dim (int): Dimension of the embedding vectors.\n",
    "\n",
    "    Returns:\n",
    "    Model: Compiled TensorFlow model.\n",
    "    \"\"\"\n",
    "    # Define the input layers\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "    # Define the embedding layers\n",
    "    user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_embedding')(user_input)\n",
    "    item_embedding = Embedding(input_dim=num_items, output_dim=embedding_dim, name='item_embedding')(item_input)\n",
    "\n",
    "    # Flatten the embeddings\n",
    "    user_vecs = Flatten()(user_embedding)\n",
    "    item_vecs = Flatten()(item_embedding)\n",
    "\n",
    "    # Concatenate the embeddings\n",
    "    concat = Concatenate()([user_vecs, item_vecs])\n",
    "\n",
    "    # Add a dense layer\n",
    "    dense = Dense(128, activation='relu')(concat)\n",
    "\n",
    "    # Add the output layer\n",
    "    output = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#Implement train_and_save_model function to train the model using the provided feature matrix X and target vector y, and save the trained model.\n",
    "\n",
    "def train_and_save_model(X, y, num_users, num_items, model_path):\n",
    "    \"\"\"\n",
    "    Train and save the recommendation model.\n",
    "\n",
    "    Args:\n",
    "    X (np.ndarray): Feature matrix.\n",
    "    y (np.ndarray): Target vector.\n",
    "    num_users (int): Number of unique users.\n",
    "    num_items (int): Number of unique items.\n",
    "    model_path (str): Path to save the trained model.\n",
    "    \"\"\"\n",
    "    # Build the model\n",
    "    model = build_model(num_users, num_items)\n",
    "\n",
    "    # Prepare input data\n",
    "    X_user = X[:, 0].astype(np.int32)\n",
    "    X_item = X[:, 1].astype(np.int32)\n",
    "    y = y.astype(np.float32)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit([X_user, X_item], y, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(model_path)\n",
    "    \n",
    "\n",
    "# Constants\n",
    "MODEL_PATH = \"path/to/save/model\"\n",
    "\n",
    "# Train and save the model\n",
    "train_and_save_model(X, y, num_users, num_items, MODEL_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c28b58-99d1-474c-a6d2-1bf8485d931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conclusion\n",
    "\"\"\"\n",
    "In this notebook, I have implemented a recommendation system using TensorFlow. Here's a summary of what I accomplished:\n",
    "\n",
    "1. Data Processing: I aggregated session-based data and engineered features like session duration, number of events, and interaction labels.\n",
    "\n",
    "2. Feature Engineering: Created mappings for user and item IDs, encoded them for model training, and generated feature vectors for users and items.\n",
    "\n",
    "3. Model Building: Constructed a recommendation model using TensorFlow, incorporating user and item embeddings, dense layers for learning interactions, and compiled it with appropriate loss and optimizer functions.\n",
    "\n",
    "4. Model Training: Trained the recommendation model using the prepared feature matrix and target vector, and saved the trained model to disk.\n",
    "\n",
    "This notebook focused on setting up the foundational components of the recommendation system, emphasizing data processing, feature engineering, and model construction. Future steps could include hyperparameter tuning, evaluation on test datasets, and deployment in real-world applications.\n",
    "\"\"\"\n",
    "\n",
    "# Save the notebook and prepare it for submission\n",
    "# Ensure all necessary libraries, functions, and explanations are included for clarity and completeness.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
